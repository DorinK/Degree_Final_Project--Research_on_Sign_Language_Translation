Stats of SLT Experiment on Phoenix14T dataset:

Epochs:
[1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10, 10, 10, 11, 11, 12, 12, 13, 13, 14, 14, 14, 15, 15, 16, 16, 17, 17, 18, 18, 19, 19, 19, 20, 20, 21, 21, 22, 22, 23, 23, 23, 24, 24, 25, 25, 26, 26, 27, 27, 28, 28, 28, 29, 29, 30, 30, 31, 31, 32, 32, 32, 33, 33, 34, 34, 35, 35, 36, 36, 37, 37, 37, 38, 38, 39, 39, 40, 40, 41, 41, 41, 42, 42, 43, 43, 44, 44, 45, 45, 46, 46, 46, 47, 47, 48, 48, 49, 49, 50, 50, 50, 51, 51, 52, 52, 53, 53, 54]

Steps:
[100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000, 2100, 2200, 2300, 2400, 2500, 2600, 2700, 2800, 2900, 3000, 3100, 3200, 3300, 3400, 3500, 3600, 3700, 3800, 3900, 4000, 4100, 4200, 4300, 4400, 4500, 4600, 4700, 4800, 4900, 5000, 5100, 5200, 5300, 5400, 5500, 5600, 5700, 5800, 5900, 6000, 6100, 6200, 6300, 6400, 6500, 6600, 6700, 6800, 6900, 7000, 7100, 7200, 7300, 7400, 7500, 7600, 7700, 7800, 7900, 8000, 8100, 8200, 8300, 8400, 8500, 8600, 8700, 8800, 8900, 9000, 9100, 9200, 9300, 9400, 9500, 9600, 9700, 9800, 9900, 10000, 10100, 10200, 10300, 10400, 10500, 10600, 10700, 10800, 10900, 11000, 11100, 11200, 11300, 11400, 11500, 11600, 11700, 11800]

Train Batch Recognition Losses:
[53.836628, 51.270096, 34.771545, 27.13829, 20.56632, 20.927635, 15.873718, 18.609383, 14.90068, 11.680498, 12.766029, 8.964961, 9.577025, 10.763535, 10.820265, 14.714436, 13.025339, 7.752283, 9.584233, 12.063466, 9.632134, 10.171709, 7.399913, 9.080071, 8.933, 10.047599, 9.388991, 7.834656, 7.863438, 7.350346, 9.254698, 8.396278, 7.314178, 6.657794, 12.513741, 6.812048, 9.934023, 8.331931, 7.019889, 5.43457, 6.734296, 9.71946, 10.13082, 6.453657, 7.470522, 9.03935, 6.853932, 6.777689, 4.586861, 3.302491, 4.419778, 5.073019, 5.710127, 4.296302, 3.721714, 5.771652, 3.711954, 3.383533, 2.843219, 1.483671, 3.06782, 6.009281, 6.006038, 2.557911, 1.618571, 3.28141, 3.188042, 2.953912, 3.294547, 3.566069, 6.5167, 2.86919, 3.156796, 1.700418, 2.975386, 1.672073, 2.657705, 2.602385, 2.630452, 1.01142, 5.168243, 2.747058, 2.839292, 2.593155, 2.833759, 1.211235, 3.603731, 1.507495, 2.146706, 4.905925, 4.448083, 1.399913, 2.053691, 1.979282, 2.4286, 3.022217, 2.87451, 2.41898, 1.72132, 1.376081, 1.898775, 2.551462, 0.926118, 1.30115, 1.686462, 3.286498, 1.548305, 0.847762, 1.580472, 2.634542, 1.003346, 1.264819, 2.179197, 1.686987, 0.600246, 1.556072, 1.290041, 1.05603]

Train Batch Translation Losses:
[62.499218, 37.871071, 40.693218, 114.470024, 33.884693, 41.861835, 29.503654, 32.149529, 33.873116, 20.499718, 27.761028, 16.08486, 17.659395, 18.16503, 20.091351, 22.387249, 40.496437, 16.911026, 13.882152, 32.696556, 23.066126, 17.600145, 13.442631, 13.622683, 13.742368, 29.367054, 16.628098, 11.375927, 11.96792, 8.132058, 16.706362, 11.659382, 19.404913, 9.601745, 38.512051, 9.523277, 11.153638, 12.955824, 12.340194, 8.265044, 10.968156, 15.974616, 23.139477, 9.726463, 8.81843, 21.75251, 9.750122, 11.650867, 3.695551, 4.923388, 5.96911, 6.190014, 8.172011, 6.694922, 5.226414, 6.625389, 4.654002, 3.064846, 3.235871, 1.670825, 3.295391, 7.398334, 7.116579, 2.539871, 2.160546, 5.182191, 2.906188, 2.968814, 3.538343, 3.664421, 9.113629, 2.594306, 2.918477, 1.326901, 2.659367, 1.751222, 2.24972, 1.7594, 2.502157, 1.241808, 5.240272, 3.09471, 1.625172, 1.624995, 2.356951, 0.775273, 2.054423, 1.218799, 1.837357, 4.506418, 4.071119, 0.882595, 1.610023, 1.355598, 1.655982, 2.575083, 2.340745, 1.382997, 1.070809, 0.582453, 1.359068, 1.548804, 0.457419, 0.671194, 1.318938, 2.049, 0.403809, 0.726194, 0.566981, 1.221009, 0.600178, 0.27696, 1.147001, 0.843087, 0.200772, 0.645318, 0.630462, 0.458044]

Dev Recognition Losses:
[945.09045, 866.18817, 590.0318, 432.564, 408.24115, 378.61975, 344.58163, 350.19727, 318.96829, 307.21265, 306.75943, 310.10382, 290.86026, 297.13934, 304.64258, 338.40591, 298.38199, 295.99115, 290.92566, 302.2753, 284.46069, 300.25278, 294.52563, 289.49408, 305.5007, 292.68008, 306.69101, 292.27374, 302.95676, 282.25406, 282.45801, 283.61816, 275.75516, 279.21777, 283.75095, 277.69217, 283.86081, 293.02234, 294.65472, 292.71228, 280.40402, 287.23285, 279.43103, 274.17996, 272.07983, 291.37115, 270.58987, 268.27374, 270.84766, 266.76273, 266.10834, 264.47876, 270.55008, 275.36777, 270.21121, 270.99252, 277.53751, 272.20572, 271.13507, 265.62845, 273.13959, 273.21799, 277.53104, 269.09735, 271.71591, 280.66229, 282.50537, 280.3927, 280.87259, 277.6676, 277.22455, 276.27332, 282.19626, 282.62231, 283.13864, 285.586, 289.56888, 288.10684, 293.34082, 286.25092, 294.1095, 292.61743, 289.57242, 290.12466, 284.58255, 290.97794, 287.64581, 293.07153, 297.18045, 295.3699, 300.89456, 297.58777, 299.73822, 302.61887, 300.76392, 304.88037, 305.23956, 303.29065, 303.65753, 306.61209, 306.05292, 310.4577, 312.49158, 310.61011, 307.71692, 310.17569, 314.08661, 310.16782, 315.24707, 313.79898, 308.95413, 313.05759, 317.06271, 315.04593, 314.47427, 313.3609, 316.4971, 318.56137]

Dev Translation Losses:
[34417.70703, 27449.29688, 25553.63086, 22675.43945, 21639.6582, 20682.46289, 20191.50781, 20193.38477, 19191.97852, 19700.75195, 19082.39844, 19095.60156, 18856.62109, 18760.55273, 18730.64844, 18634.94727, 18901.82617, 18734.875, 18795.34375, 18883.73438, 19119.49414, 19108.5625, 19234.33398, 19114.87109, 19403.00781, 19611.33984, 19497.71875, 19694.11914, 19776.10742, 20042.62109, 20487.74219, 20443.24023, 20020.9707, 20433.16406, 20753.77734, 20871.7793, 20861.45508, 20822.48828, 21439.00195, 21322.06836, 21097.69336, 21284.46289, 21668.02539, 21794.42383, 22015.0293, 22240.54492, 21730.93555, 21844.56055, 22202.67188, 22452.71289, 22432.74609, 22950.70508, 22992.44922, 23232.52344, 23276.97656, 23739.8418, 23968.93945, 23759.71094, 23698.69922, 23900.11719, 24192.97852, 24328.46094, 24585.40234, 24925.79492, 24736.4082, 24921.54297, 25175.35352, 25359.6582, 25845.48242, 25540.49414, 25248.5957, 25569.3125, 25595.08398, 25638.29688, 25827.88867, 26061.32422, 26269.10352, 26283.97852, 26559.50781, 26447.43164, 26503.89062, 26701.15234, 26528.17773, 26539.77539, 26684.90625, 26727.25391, 26777.08008, 26799.22461, 26866.54883, 26869.42578, 27018.02148, 27064.40234, 27159.62695, 27260.64453, 27313.7168, 27296.86914, 27343.49805, 27467.10547, 27162.71875, 27223.33203, 27451.20312, 27356.98242, 27426.0918, 27395.62109, 27445.35742, 27368.05078, 27295.92969, 27415.28125, 27335.91797, 27400.66406, 27363.5625, 27560.40039, 27386.58789, 27362.80469, 27344.70703, 27783.62695, 27466.81836, 27597.62305]

WER:
[100.0, 96.9, 83.88, 71.9, 54.52, 48.57, 46.14, 48.23, 40.7, 45.53, 38.7, 39.79, 37.1, 35.87, 41.69, 32.27, 39.47, 38.91, 36.51, 40.94, 35.47, 38.56, 36.24, 35.9, 37.36, 34.83, 39.26, 33.79, 35.95, 33.97, 34.88, 36.43, 33.71, 33.01, 33.97, 32.48, 32.24, 32.61, 35.39, 36.91, 33.09, 33.92, 33.2, 31.36, 32.0, 34.69, 30.08, 30.72, 32.03, 31.55, 30.72, 29.57, 29.3, 29.78, 30.56, 30.1, 31.76, 29.92, 28.96, 30.1, 30.16, 28.8, 30.24, 28.88, 29.09, 28.53, 29.33, 28.96, 29.01, 29.04, 28.48, 28.21, 29.17, 28.66, 27.94, 28.5, 29.04, 28.5, 28.66, 29.36, 29.28, 28.53, 28.02, 28.5, 28.66, 28.74, 27.81, 28.29, 28.42, 28.5, 28.88, 29.73, 28.64, 28.98, 28.1, 28.8, 29.44, 28.56, 28.66, 28.77, 28.93, 28.69, 28.69, 28.66, 28.66, 28.37, 28.13, 28.05, 28.77, 29.01, 27.76, 28.4, 28.13, 28.29, 28.05, 27.94, 28.1, 28.1]

BLEU-4:
[3.85, 4.12, 5.43, 9.72, 11.81, 12.79, 13.07, 12.87, 14.56, 17.02, 15.61, 16.15, 16.61, 17.09, 16.75, 17.42, 17.03, 18.47, 18.56, 17.15, 18.25, 16.73, 17.93, 19.13, 18.97, 18.2, 19.45, 18.81, 18.51, 18.68, 16.97, 18.36, 19.49, 18.78, 17.03, 19.48, 19.67, 19.19, 18.35, 18.67, 19.17, 18.47, 18.24, 18.52, 18.62, 18.32, 19.92, 20.17, 20.24, 19.51, 20.06, 19.05, 19.67, 19.32, 18.86, 20.12, 19.99, 19.83, 19.99, 19.97, 20.49, 20.04, 19.56, 19.85, 19.87, 19.6, 19.8, 20.35, 19.51, 19.06, 19.68, 19.72, 20.82, 20.79, 20.62, 20.42, 19.99, 20.68, 20.07, 19.92, 19.92, 19.61, 20.61, 19.73, 20.27, 19.64, 19.99, 20.44, 20.84, 20.01, 20.28, 20.7, 20.8, 19.95, 19.82, 19.92, 19.97, 20.31, 20.26, 21.29, 20.48, 20.67, 20.98, 21.04, 20.75, 20.82, 20.31, 20.37, 21.1, 20.6, 20.83, 20.51, 20.65, 20.23, 20.8, 20.61, 21.05, 20.38]